{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gathering the data for this project was a mix of downloading files that were given to us and programatically downloading files off of the internet via a link. They are listed below.\n",
    ">\n",
    "> - <b>twitter_archive_enhanced.csv</b>\n",
    ">\n",
    ">    > This was a file that was given to us from Udacity and was to be downloaded locally on our own machine and uploaded to the jupyter notebook virtual machine. This file contained 5000+ tweets from the WeRateDogs twitter archive. Some of the columns included were the full text of the tweet, name, rating (numerator/denominator), and the dog \"stage\" (doggo, floofer, pupper, and puppo).\n",
    ">\n",
    "> - <b>image_predictions.tsv</b>\n",
    ">\n",
    ">    > This was a file that we had to download programatically from the Udacity servers and store in a file on the jupyter virtual machine. This was a file that included images (that users uploaded with the tweets) that were run through a neural network algorithm, the algorithm predicted what dog breed the dog was based on the tweet picture. It also included confidence levels of the algorithm in being able to match what dog breed the dog was.\n",
    ">\n",
    "> - <b>twitter_api.py</b>\n",
    ">\n",
    ">    > This was a file that described how to query the twitter api with access tokens. To get these you would have to sign up for twitter. I did not go this route because I do not have social media and did not want to start a twitter account. I used the option below.\n",
    ">\n",
    "> - <b>tweet_json.txt</b>\n",
    "> \n",
    ">    > This was a file that was programatically downloaded and stored as a file on the jupyter virtual machine. This file was a json file that included text data that needed to be extracted. It included tweet data about each tweet, around 5000 of them. Some columns included user id, retweet count, favorite count, the full text of each tweet and the text character range of each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assesment of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After loading in each file as a datafame, I visually and programatically assesed each one. Visually by going through each dataframe and scanning for anything that may need to be change. Programatically by using tools, i.e. info(), describe(), isnull(), duplicated(), and value_counts(). My assessment is below.\n",
    ">\n",
    ">\n",
    ">\n",
    ">\n",
    "### Quality Issues\n",
    ">\n",
    "> <b>df table</b>\n",
    ">\n",
    "> - Timestamp column should be datetime data type.\n",
    ">\n",
    "> - Combine the columns 'doggo', 'floofer', 'pupper' and 'puppo' into one column called, \"Dog Rating\".\n",
    ">\n",
    "> - Drop Columns that are not needed. \n",
    ">\n",
    "> <b>image_pred</b>\n",
    ">\n",
    "> - Column headers need to be more consitent and descriptive.\n",
    "> \n",
    "> - Collect all non-dog breed predictions and drop them.\n",
    "> \n",
    "> - Standardize all dog names in 'Breed Prediction 1/2/3'. Capitalize first letters and remove underscores.\n",
    ">\n",
    "> - Drop duplicates in jpg_urls column.\n",
    ">\n",
    "> - Drop the last 7 columns, they are not needed. We only need to know what dog breed the algorithm was most confident in.\n",
    ">\n",
    "> <b>tweet_df</b>\n",
    ">\n",
    "> - The 'retweeted_status' column tells us that those are the tweets that are retweeted, the ones without 'NaaN' values need to go. \n",
    ">\n",
    "> - Drop 'retweeted_status' column as it is not necessary.\n",
    ">\n",
    "> - Renaming 'id' column to 'tweet_id'.\n",
    ">\n",
    ">\n",
    "### Tidiness issues\n",
    "> \n",
    "> - Merge all of the tables into one master dataframe.\n",
    "> \n",
    "> - Dropping columns that are not needed, renaming columns, as well as combining columns (all referred to above).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As anyone who works with data know, cleaning takes 90% of your time. I had previous experience cleaning but I was introduced to some challenging new functions I had never worked with. Melt() being one of them. I had to melt() three columns into one and not include other columns. Another issue I had was trying to drop rows that were NOT NaN as well as dropping columns based on boolean conditions. Merging tables was also an issue- When I started merging I found that certain columns that contained NaN values that shouldn't be there. When I went back and looked I figured out that I hadn't dropped the rows that didn't have images (Udacity wanted this), so it was creating more rows that didn't match. Once I did that, it merged perfectly. All in all this was a challenging course but I find that the better I get with cleaning the better I am at understanding the data and not making mistakes I had previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once I had all of my tables merged, I was able to store them in a file called twitter_archive_master.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
